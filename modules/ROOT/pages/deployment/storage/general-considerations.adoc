= General Storage Considerations and Settings
:toc: right
:toclevels: 2
:description: Infinite Scale can connect to several types of storages. This document gives some general information, considerations and recommendations with regards to supported storages.

== Introduction

{description}

Note that depending on the storage used, special settings can be defined to optimize resources for memory and performance.

For a quick and rough overview, the following storages are mapped against an expected sizing. Note that the way how Infinite Scale is setup like xref:depl-examples/bare-metal.adoc[binary] or xref:deployment/container/orchestration/orchestration.adoc[orchestration] with its variants usually comes along with the expected sizing:

{empty} +

[role=center,width=100%,cols="25%,45%,45%",options="header"]
|===
| Size
| Case
| Storage Type

| Small or testing
| Home
| Local Storage

| Medium
| Small and midmarket enterprises
| NFS

| Large
| Large enterprises and hosters
| S3 for blobs +
NFS for metadata and other data
|===

[NOTE]
====
* This page intends to give an overview and does not go into configuration details.
* Any level of space requirement described below shall give an direction what to expect, but never a number.
====

== Write Consistency

Independent of the storage used and how many instances of the xref:{s-path}/storage-users.adoc[storage-users] service are running, Infinite Scale has mechanisms implemented to guarantee consistent writing to the backend for distributed setups. In any case, a file ID based on UUIDv4 is created to guarantee the uniqueness of files names. A proper file locking is implemented for writing metadata.

== Local Storage

Using local storage is a good choice if you want to test or plan to have a small productive instance. As drawback, scalability and failover is because of one monolithic block not given. Note that you can at anytime switch selected mount points defined to NFS which is described in the next section.

To decide which filesystem for local storage can be used, see the  xref:prerequisites/prerequisites.adoc#supported-posix-compliant-filesystems[Supported POSIX-compliant Filesystems].

We highly recommend, if the instance is planned to be used in production, to consider the follwing topics. For details and which environment variables necessary for the changes see the xref:deployment/general/general-info.adoc#default-paths[Default Paths] documentation:

* To avoid filling up the filesystem and making the system unresponsive:
** Separate via partitions the OS from where Infinite Scale metadata and blobs will be stored.

* If you expect a data volume that can lead to a high demand in *search* and/or *thumbnail* space requirements - which would be stored without manual configuration in a subfolder using the `OCIS_BASE_DATA_PATH` as base path.
** Though not required, the data path for services mentioned in xref:deployment/general/general-info.adoc#base-data-directory[Services That Can Deviate from the Base Data Path] can be deviated and individually configured. Using an additional partition/moint point is a possible way to achieve this.

The following table describes where data should be stored when using Infinite Scale in production:

{empty} +

[role=center,width=70%,cols="40%,45%,60%",options="header"]
|===
| Envorinment Variable
| Target
| Space requirement

| `OCIS_CONFIG_DIR`
| Default (local)
| Very low

| `OCIS_BASE_DATA_PATH`
| Partition
| Medium
|===

== NFS

Using NFS is benefical because the instance components are separated from the storage and can scale therefore independently. Such a setup is used when you plan a production environment with many users. As the same rules apply described in the topics of xref:local-storage[Local Storage], you mount instead using partitions NFS mount points. In addition, the Infinite Scale config is also stored on NFS using the xref:deployment/general/general-info.adoc#default-paths[OCIS_CONFIG_DIR] environment variable as this is a requirement for xref:deployment/container/orchestration/orchestration.adoc[orchestrated environments]. For details on NFS see the xref:deployment/storage/nfs.adoc[Network File System] documentation.

{empty} +

[role=center,width=70%,cols="40%,35%,80%",options="header"]
|===
| Envorinment Variable
| Target
| Space requirement

| `OCIS_CONFIG_DIR`
| NFS_1
| Very low

| `OCIS_BASE_DATA_PATH`
| NFS_2
| High

| Seach and Thumbnails
| NFS_3
| Medium, to be monitored
|===

== S3

S3 to store blobs is typically used by large enterprises and hosters, though it can fit for medium enterprises too. Data distribution and separation is a bit different compared to a pure POSIX backend. For details on S3 see the xref:deployment/storage/s3.adoc[S3] documentation. Also see the xref:deployment/general/general-info.adoc#using-s3-for-blobs[Using S3 for Blobs] description:

* POSIX storage, usually NFS.
** Metadata
** Data for search and/or thumbnails
** Other data

* S3 for blobs

As you can see in the table below, the layout distributes data over different storages and mounts based on their use case. This is by nature to enable scaling the system according the needs of large enterprises.

{empty} +

[role=center,width=90%,cols="40%,45%,80%",options="header"]
|===
| Envorinment Variable
| Target
| Space requirement

| `OCIS_CONFIG_DIR`
| NFS_1
| Very low

| `OCIS_BASE_DATA_PATH`
| NFS_2
| Medium, to be monitored +
Alternatively low if NFS_4

| Seach and Thumbnails
| NFS_3
| Medium, to be monitored

| `STORAGE_USERS_S3NG_ROOT`
| `OCIS_BASE_DATA_PATH` +
Alternatively NFS_4
| Medium, to be monitored

| S3 specific settings
| S3
| High
|===

== Resource Optimisation

Depending on the storage connected and the servers capabilities, Infinite Scale can be optimized using the servers resources. The relevant environment variable to configure this is:

`STORAGE_USERS_OCIS_MAX_CONCURRENCY`

The value to consider is based on how much CPU's and memory the server has the instance of the xref:{s-path}/storage-users.adoc[storage-users] service is running on, which kind of storage, POSIX or S3 is used for blobs and what you want to achieve.

=== Background

In a nutshell, the value for `STORAGE_USERS_OCIS_MAX_CONCURRENCY` defines how many workers are assigned to storage related tasks. Any worker not only serves its job, but also consumes CPU and memory resources which needs to balance out. On the other hand side, when it comes to the connected storage, workers serving S3 will be more in response waiting time compared to POSIX connections. As workers which are in waiting state do consume less resources, the value can be considered to allow overcommitting CPU resources.

=== Value Considerations

The following considerations are based on using POSIX storage only

* Performance without worrying about memory +
`runtime.NumCPU() * 2`
* Performance +
`runtime.NumCPU()`
* Limited memory available +
 A value of 4 or lower, assuming 4 is still lower than the number of CPU available

If S3 is used storing blobs, the resulting value can be increased.

NOTE: It is essential to monitor you instance with respect to CPU, memory, network latency and the load pattern created by users. Only this can give you a final view on adapting the value.
